"""Test step implementations with parallelized verification and robust sync handling."""

import asyncio
import time
import os
import httpx
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple

from monke.core.config import TestConfig
from monke.utils.logging import get_logger


class TestStep(ABC):
    """Abstract base class for test steps."""

    def __init__(self, config: TestConfig) -> None:
        """Initialize the test step."""
        self.config = config
        self.logger = get_logger(f"test_step.{self.__class__.__name__}")

    def _display_name(self, entity: Dict[str, Any]) -> str:
        """Return a human-readable identifier for an entity regardless of type."""
        return (
            entity.get("path")
            or entity.get("title")
            or entity.get("id")
            or entity.get("url")
            or "<unknown>"
        )

    @abstractmethod
    async def execute(self) -> None:
        """Execute the test step."""
        raise NotImplementedError


class CreateStep(TestStep):
    """Create test entities step."""

    async def execute(self) -> None:
        """Create test entities via the connector."""
        self.logger.info("ü•Å Creating test entities")

        bongo = self._get_bongo()
        entities = await bongo.create_entities()

        # Optional post-create delay to allow upstream APIs to propagate data
        delay_seconds = 0
        try:
            delay_override = (
                self.config.connector.config_fields.get("post_create_sleep_seconds")
                if hasattr(self.config, "connector")
                and hasattr(self.config.connector, "config_fields")
                else None
            )
            if delay_override is not None:
                delay_seconds = int(delay_override)
        except Exception:
            delay_seconds = 0

        if delay_seconds > 0:
            self.logger.info(
                f"‚è∏Ô∏è Waiting {delay_seconds}s after creation to allow source API propagation"
            )
            await asyncio.sleep(delay_seconds)

        self.logger.info(f"‚úÖ Created {len(entities)} test entities")

        # Store entities for later steps and on bongo for deletes
        self.config._created_entities = entities
        if hasattr(self.config, "_bongo"):
            self.config._bongo.created_entities = entities

    def _get_bongo(self) -> Optional[Any]:
        return getattr(self.config, "_bongo", None)


class SyncStep(TestStep):
    """Sync data to Airweave step."""

    async def execute(self) -> None:
        """Trigger sync and wait for completion."""
        self.logger.info("üîÑ Syncing data to Airweave")

        # If a job is already running, wait for it, BUT ALWAYS launch our own sync afterwards
        active_job_id = self._find_active_job_id()
        if active_job_id:
            self.logger.info(
                f"üü° A sync is already in progress (job {active_job_id}); waiting for it to complete."
            )
            await self._wait_for_sync_completion(target_job_id=active_job_id)
            self.logger.info(
                "üß≠ Previous sync finished; launching a fresh sync to capture recent changes"
            )

        # Try to start a new sync. If the server says one is already running, wait for that one,
        # then START OUR OWN sync and wait for it too.
        target_job_id: Optional[str] = None
        try:
            run_resp = self._http_post(
                f"/source-connections/{self.config._source_connection_id}/run",
                json=None,
            )
            target_job_id = str(run_resp["id"])
        except Exception as e:
            msg = str(e).lower()
            if "already has a running job" in msg or "already running" in msg:
                self.logger.warning("‚ö†Ô∏è Sync already running; discovering and waiting for that job.")
                active_job_id = self._find_active_job_id() or self._get_latest_job_id()
                if not active_job_id:
                    # Last resort: brief wait then re-check
                    await asyncio.sleep(2.0)
                    active_job_id = self._find_active_job_id() or self._get_latest_job_id()
                if not active_job_id:
                    raise  # nothing to wait on; re-raise original error
                await self._wait_for_sync_completion(target_job_id=active_job_id)

                # IMPORTANT: after the previous job completes, start *our* job
                run_resp = self._http_post(
                    f"/source-connections/{self.config._source_connection_id}/run",
                    json=None,
                )
                target_job_id = str(run_resp["id"])
            else:
                raise  # unknown error

        await self._wait_for_sync_completion(target_job_id=target_job_id)
        self.logger.info("‚úÖ Sync completed")

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)

    def _get_jobs(self) -> List[Dict[str, Any]]:
        """Get list of sync jobs for the source connection, sorted by recency."""
        jobs = self._http_get(f"/source-connections/{self.config._source_connection_id}/jobs") or []
        # Sort by started_at or created_at, newest first
        return sorted(
            jobs, key=lambda j: j.get("started_at") or j.get("created_at") or "", reverse=True
        )

    def _find_active_job_id(self) -> Optional[str]:
        """Find an active job from the jobs list."""
        ACTIVE = {"created", "pending", "in_progress", "running", "queued"}
        jobs = self._get_jobs()
        for job in jobs:
            if job.get("status", "").lower() in ACTIVE:
                return str(job["id"])
        return None

    def _get_latest_job_id(self) -> Optional[str]:
        """Get the latest job ID."""
        jobs = self._get_jobs()
        if jobs:
            return str(jobs[0]["id"])
        return None

    async def _wait_for_sync_completion(
        self,
        target_job_id: Optional[str] = None,
        timeout_seconds: int = 300,
    ) -> None:
        """Wait for sync job to complete."""
        self.logger.info("‚è≥ Waiting for sync to complete...")

        ACTIVE_STATUSES = {"created", "pending", "in_progress", "running", "queued"}

        # Find job ID if not provided
        if not target_job_id:
            target_job_id = self._find_active_job_id()

        # Still no job? Wait for one to appear
        if not target_job_id:
            self.logger.info("‚ÑπÔ∏è No job id available; waiting for new job...")
            start = time.monotonic()
            prev_latest = (
                self.config._last_sync_job_id if hasattr(self.config, "_last_sync_job_id") else None
            )

            while time.monotonic() - start < timeout_seconds:
                # Try to get latest job
                latest_id = self._get_latest_job_id()
                if latest_id and latest_id != prev_latest:
                    target_job_id = latest_id
                    self.logger.info(f"üÜî Detected sync job id: {target_job_id}")
                    break
                await asyncio.sleep(2.0)

            if not target_job_id:
                raise RuntimeError("Couldn't obtain a sync job id to wait on.")

        # Poll for job completion
        start = time.monotonic()
        while time.monotonic() - start < timeout_seconds:
            # Find the job in our jobs list
            job = None
            try:
                jobs = self._get_jobs()
                for j in jobs:
                    if str(j["id"]) == str(target_job_id):
                        job = j
                        break
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Error fetching job status: {e}")

            if not job:
                await asyncio.sleep(2.0)
                continue

            # Check job status
            status = job.get("status", "").lower()
            completed_at = job.get("completed_at")
            error = job.get("error")

            self.logger.info(f"üîç Job {target_job_id} status={status}, completed_at={completed_at}")

            # Check for failure
            if status == "failed":
                raise RuntimeError(f"Sync failed: {error or 'unknown error'}")

            # Check for completion
            if status == "completed" and completed_at:
                self.config._last_sync_job_id = str(target_job_id)
                self.logger.info("‚úÖ Sync completed successfully")
                return

            # Still running
            if status in ACTIVE_STATUSES:
                await asyncio.sleep(2.0)
                continue

            # Unexpected state
            await asyncio.sleep(0.5)

        raise TimeoutError("Sync timeout reached")

    # ----- HTTP helpers (avoid SDK schema mismatches during transition) -----
    def _http_headers(self) -> Dict[str, str]:
        headers = {"Content-Type": "application/json"}
        api_key = os.getenv("AIRWEAVE_API_KEY")
        if api_key:
            headers["x-api-key"] = api_key
        return headers

    def _base_url(self) -> str:
        return os.getenv("AIRWEAVE_API_URL", "http://localhost:8001").rstrip("/")

    def _http_get(self, path: str) -> Any:
        resp = httpx.get(f"{self._base_url()}{path}", headers=self._http_headers(), timeout=30.0)
        resp.raise_for_status()
        return resp.json()

    def _http_post(self, path: str, json: Optional[Dict[str, Any]]) -> Any:
        resp = httpx.post(
            f"{self._base_url()}{path}",
            headers=self._http_headers(),
            json=json,
            timeout=30.0,
        )
        resp.raise_for_status()
        return resp.json()


# ---------- Shared search helpers ----------


def _safe_results_from_search_response(resp) -> List[Dict[str, Any]]:
    """
    Accept either a Pydantic model or plain dict. Return list of result dicts.
    """
    if resp is None:
        return []

    try:
        data = resp.model_dump()
    except AttributeError:
        try:
            data = dict(resp)
        except Exception:
            data = {}

    results = data.get("results")
    if results is None and "items" in data:
        results = data["items"]

    if isinstance(results, list):
        return results
    return []


async def _search_collection_async(
    client, readable_id: str, query: str, limit: int = 1000
) -> List[Dict[str, Any]]:
    """
    Use Airweave's advanced search API endpoint with all extra features disabled.
    Always uses a limit of 1000 for comprehensive results.
    """
    import aiohttp
    import os

    # Build the search request with all extra features disabled
    search_request = {
        "query": query,
        "limit": 1000,  # Always use 1000 for comprehensive results
        "response_type": "raw",
        "enable_reranking": False,
        "enable_query_interpretation": False,
        "expansion_strategy": "no_expansion",
        "search_method": "keyword",  # Use keyword search for exact string matching
    }

    # Get the API URL and key
    api_url = os.getenv("AIRWEAVE_API_URL", "http://localhost:8001")
    api_key = os.getenv("AIRWEAVE_API_KEY")

    headers = {"Content-Type": "application/json"}
    if api_key:
        headers["X-API-Key"] = api_key

    url = f"{api_url}/collections/{readable_id}/search"

    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(url, json=search_request, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("results", [])
                else:
                    return []
        except Exception:
            return []


async def _token_present_in_collection(
    client, readable_id: str, token: str, limit: int = 1000
) -> bool:
    """
    Check if `token` appears in any result payload (case-insensitive).
    Uses a fixed limit of 1000 for comprehensive search.
    """
    try:
        # Always use 1000 limit for comprehensive results
        results = await _search_collection_async(client, readable_id, token, 1000)
        token_lower = token.lower()
        for r in results:
            payload = r.get("payload", {})
            if payload and token_lower in str(payload).lower():
                return True
        return False
    except Exception as e:
        get_logger("monke").error(f"Error checking token present in collection: {e}")
        return False


def _search_limit_from_config(config: TestConfig, default: int = 50) -> int:
    """Get search limit from config or use default."""
    try:
        return int(config.verification_config.get("search_limit", default))
    except Exception:
        return default


# ---------- Verification steps (parallelized) ----------


class VerifyStep(TestStep):
    """Verify data in Qdrant step."""

    async def execute(self) -> None:
        self.logger.info("üîç Verifying entities in Qdrant")
        client = self._get_airweave_client()

        async def verify_one(entity: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
            expected_token = entity.get("token")
            if not expected_token:
                self.logger.warning("‚ö†Ô∏è No token found in entity, falling back to filename")
                expected_token = (entity.get("path") or "").split("/")[-1]

            # Always use 1000 limit for comprehensive search
            ok = await _token_present_in_collection(
                client, self.config._collection_readable_id, expected_token, 1000
            )
            return entity, ok

        # Retry support + optional one-time rescue resync
        attempts = int(self.config.verification_config.get("retries", 5))
        backoff = float(self.config.verification_config.get("retry_backoff_seconds", 1.0))
        resync_on_miss = bool(self.config.verification_config.get("resync_on_miss", True))

        resync_lock = asyncio.Lock()
        resync_triggered = False

        async def verify_with_retries(e: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
            nonlocal resync_triggered

            for i in range(max(1, attempts)):
                entity, ok = await verify_one(e)
                if ok:
                    return entity, True
                await asyncio.sleep(backoff)

            if resync_on_miss:
                async with resync_lock:
                    if not resync_triggered:
                        resync_triggered = True
                        self.logger.info(
                            "üîÅ Miss detected during verify; triggering an extra sync ‚Ä¶"
                        )
                        # Reuse the same SyncStep logic to avoid duplication
                        await SyncStep(self.config).execute()
                # Final check after resync
                return await verify_one(e)

            return e, False

        results = await asyncio.gather(
            *[verify_with_retries(e) for e in self.config._created_entities]
        )

        errors = []
        for entity, ok in results:
            if not ok:
                errors.append(f"Entity {self._display_name(entity)} not found in Qdrant")
            else:
                self.logger.info(f"‚úÖ Entity {self._display_name(entity)} verified in Qdrant")

        if errors:
            raise Exception("; ".join(errors))

        self.logger.info("‚úÖ All entities verified in Qdrant")

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)


class UpdateStep(TestStep):
    """Update test entities step."""

    async def execute(self) -> None:
        self.logger.info("üìù Updating test entities")
        bongo = self._get_bongo()
        updated_entities = await bongo.update_entities()
        self.logger.info(f"‚úÖ Updated {len(updated_entities)} test entities")
        self.config._updated_entities = updated_entities

    def _get_bongo(self) -> Optional[Any]:
        return getattr(self.config, "_bongo", None)


class PartialDeleteStep(TestStep):
    """Partial deletion step - delete subset of entities based on test size."""

    async def execute(self) -> None:
        self.logger.info("üóëÔ∏è Executing partial deletion")
        bongo = self._get_bongo()

        deletion_count = self._calculate_partial_deletion_count()
        entities_to_delete = self.config._created_entities[:deletion_count]
        entities_to_keep = self.config._created_entities[deletion_count:]

        self.logger.info(
            f"üóëÔ∏è Deleting {len(entities_to_delete)} entities: "
            f"{[self._display_name(e) for e in entities_to_delete]}"
        )
        self.logger.info(
            f"üíæ Keeping {len(entities_to_keep)} entities: "
            f"{[self._display_name(e) for e in entities_to_keep]}"
        )

        deleted_paths = await bongo.delete_specific_entities(entities_to_delete)

        self.config._partially_deleted_entities = entities_to_delete
        self.config._remaining_entities = entities_to_keep

        self.logger.info(f"‚úÖ Partial deletion completed: {len(deleted_paths)} entities deleted")

    def _get_bongo(self) -> Optional[Any]:
        return getattr(self.config, "_bongo", None)

    def _calculate_partial_deletion_count(self) -> int:
        return self.config.deletion.partial_delete_count


class VerifyPartialDeletionStep(TestStep):
    """Verify that partially deleted entities are removed from Qdrant."""

    async def execute(self) -> None:
        self.logger.info("üîç Verifying partial deletion")

        if not self.config.deletion.verify_partial_deletion:
            self.logger.info("‚è≠Ô∏è Skipping partial deletion verification (disabled in config)")
            return

        client = self._get_airweave_client()

        self.logger.info("üîç Expecting these entities to be deleted:")
        for entity in self.config._partially_deleted_entities:
            self.logger.info(
                f"   - {self._display_name(entity)} (token: {entity.get('token', 'N/A')})"
            )

        async def check_deleted(entity: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
            # Prefer searching with the most specific identifier available
            search_query = (
                str(entity.get("id") or "")
                or str(entity.get("gid") or "")
                or str(entity.get("token") or "")
                or str(entity.get("name") or "")
                or (str(entity.get("path") or "").split("/")[-1])
                or str(entity.get("url") or "")
            )
            if not search_query:
                return entity, False

            # Always use 1000 limit for comprehensive search
            results = await _search_collection_async(
                client, self.config._collection_readable_id, search_query, 1000
            )

            def _values_equal(a: Any, b: Any) -> bool:
                return str(a) == str(b)

            # First, try to prove presence via exact field equality on common identifiers
            keys_to_check = [
                "id",
                "gid",
                "token",
                "path",
                "url",
                "external_id",
                "name",
            ]
            present = False
            for r in results:
                payload = r.get("payload", {}) or {}

                # Exact match on any known identifier
                for k in keys_to_check:
                    ent_val = entity.get(k)
                    pay_val = payload.get(k)
                    if ent_val and pay_val and _values_equal(ent_val, pay_val):
                        present = True
                        break
                if present:
                    break

                # Fallback: substring match on sufficiently long tokens only
                token_val = entity.get("token")
                if token_val and len(str(token_val)) >= 12:
                    if str(token_val).lower() in str(payload).lower():
                        present = True
                        break

            return entity, (not present)

        results = await asyncio.gather(
            *[check_deleted(e) for e in self.config._partially_deleted_entities]
        )

        errors = []
        for entity, is_removed in results:
            if not is_removed:
                errors.append(
                    f"Entity {self._display_name(entity)} still exists in Qdrant after deletion"
                )
            else:
                self.logger.info(
                    f"‚úÖ Entity {self._display_name(entity)} confirmed removed from Qdrant"
                )

        if errors:
            raise Exception("; ".join(errors))

        self.logger.info("‚úÖ Partial deletion verification completed")

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)


class VerifyRemainingEntitiesStep(TestStep):
    """Verify that remaining entities are still present in Qdrant."""

    async def execute(self) -> None:
        self.logger.info("üîç Verifying remaining entities are still present")

        if not self.config.deletion.verify_remaining_entities:
            self.logger.info("‚è≠Ô∏è Skipping remaining entities verification (disabled in config)")
            return

        client = self._get_airweave_client()

        async def check_present(entity: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:
            expected_token = entity.get("token") or (
                (entity.get("path", "").split("/")[-1])
                if entity.get("path")
                else str(entity.get("id", ""))
            )
            if not expected_token:
                return entity, False
            # Always use 1000 limit for comprehensive search
            present = await _token_present_in_collection(
                client, self.config._collection_readable_id, expected_token, 1000
            )
            return entity, present

        results = await asyncio.gather(*[check_present(e) for e in self.config._remaining_entities])

        errors = []
        for entity, is_present in results:
            if not is_present:
                errors.append(
                    f"Entity {self._display_name(entity)} was incorrectly removed from Qdrant"
                )
            else:
                self.logger.info(
                    f"‚úÖ Entity {self._display_name(entity)} confirmed still present in Qdrant"
                )

        if errors:
            raise Exception("; ".join(errors))

        self.logger.info("‚úÖ Remaining entities verification completed")

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)


class CompleteDeleteStep(TestStep):
    """Complete deletion step - delete all remaining entities."""

    async def execute(self) -> None:
        self.logger.info("üóëÔ∏è Executing complete deletion")

        bongo = self._get_bongo()

        remaining_entities = self.config._remaining_entities
        if not remaining_entities:
            self.logger.info("‚ÑπÔ∏è No remaining entities to delete")
            return

        self.logger.info(f"üóëÔ∏è Deleting remaining {len(remaining_entities)} entities")

        deleted_paths = await bongo.delete_specific_entities(remaining_entities)

        self.logger.info(f"‚úÖ Complete deletion completed: {len(deleted_paths)} entities deleted")

    def _get_bongo(self) -> Optional[Any]:
        return getattr(self.config, "_bongo", None)


class VerifyCompleteDeletionStep(TestStep):
    """Verify that all test entities are completely removed from Qdrant."""

    async def execute(self) -> None:
        self.logger.info("üîç Verifying complete deletion")

        if not self.config.deletion.verify_complete_deletion:
            self.logger.info("‚è≠Ô∏è Skipping complete deletion verification (disabled in config)")
            return

        client = self._get_airweave_client()

        all_test_entities = (
            self.config._partially_deleted_entities + self.config._remaining_entities
        )

        async def check_deleted(entity: Dict[str, Any]) -> Tuple[Dict[str, Any], bool]:

            # Get token to search for
            expected_token = entity.get("token") or (
                (entity.get("path", "").split("/")[-1])
                if entity.get("path")
                else str(entity.get("id", ""))
            )

            if not expected_token:
                return entity, False

            # Always use 1000 limit for comprehensive search
            present = await _token_present_in_collection(
                client, self.config._collection_readable_id, expected_token, 1000
            )

            if present:
                # Let's see what was found
                self.logger.warning(
                    f"‚ö†Ô∏è Entity {self._display_name(entity)} still found with token: {expected_token}"
                )
                # Do a more detailed search to see what's in Qdrant
                try:
                    results = await _search_collection_async(
                        client, self.config._collection_readable_id, expected_token, 5
                    )
                    for r in results[:2]:  # Show first 2 results
                        payload = r.get("payload", {})
                        self.logger.info(
                            f"   Found in Qdrant: id={payload.get('id')}, name={payload.get('name')}"
                        )
                except Exception as e:
                    self.logger.debug(f"Could not get detailed results: {e}")

            return entity, (not present)

        results = await asyncio.gather(*[check_deleted(e) for e in all_test_entities])

        errors = []
        for entity, is_removed in results:
            if not is_removed:
                errors.append(
                    f"Entity {self._display_name(entity)} still exists in Qdrant after complete deletion"
                )
            else:
                self.logger.info(
                    f"‚úÖ Entity {self._display_name(entity)} confirmed removed from Qdrant"
                )

        if errors:
            raise Exception("; ".join(errors))

        # Always use 1000 limit for comprehensive search
        collection_empty = await self._verify_collection_empty_of_test_data(client, 1000)
        if not collection_empty:
            self.logger.warning(
                "‚ö†Ô∏è Qdrant collection still contains some data (may be metadata entities)"
            )
        else:
            self.logger.info("‚úÖ Qdrant collection confirmed empty of test data")

        self.logger.info("‚úÖ Complete deletion verification completed")

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)

    async def _verify_collection_empty_of_test_data(self, client: Any, limit: int) -> bool:
        try:
            test_patterns = ["monke-test", "Monke Test"]

            async def search_one(pattern: str) -> Tuple[str, List[Dict[str, Any]]]:
                try:
                    results = await _search_collection_async(
                        client,
                        self.config._collection_readable_id,
                        pattern,
                        limit=min(limit, 25),
                    )
                    return pattern, results
                except Exception:
                    return pattern, []

            pattern_results = await asyncio.gather(*[search_one(p) for p in test_patterns])

            total = 0
            for pattern, results in pattern_results:
                count = len(results)
                total += count
                if count:
                    self.logger.info(f"üîç Found {count} results for pattern '{pattern}'")
                    for r in results[:3]:
                        payload = r.get("payload", {})
                        score = r.get("score")
                        self.logger.info(f"   - {payload.get('name', 'Unknown')} (score: {score})")

            if total == 0:
                self.logger.info("‚úÖ No test data found in collection")
                return True
            else:
                self.logger.warning(f"‚ö†Ô∏è Found {total} test data results in collection")
                return False

        except Exception as e:
            self.logger.error(f"‚ùå Error verifying collection emptiness: {e}")
            return False


class CleanupStep(TestStep):
    """Cleanup step - clean up entire source workspace."""

    async def execute(self) -> None:
        """Clean up all test data from the source workspace."""
        self.logger.info("üßπ Cleaning up source workspace")
        bongo = self._get_bongo()

        try:
            await bongo.cleanup()
            self.logger.info("‚úÖ Source workspace cleanup completed")
        except Exception as e:
            # Don't fail the test if cleanup fails, just log the warning
            self.logger.warning(f"‚ö†Ô∏è Cleanup encountered issues: {e}")

    def _get_bongo(self) -> Optional[Any]:
        return getattr(self.config, "_bongo", None)


class CollectionCleanupStep(TestStep):
    """Collection cleanup step - clean up old test collections from Airweave."""

    async def execute(self) -> None:
        """Clean up old test collections from Airweave."""
        self.logger.info("üßπ Cleaning up old test collections")
        client = self._get_airweave_client()

        if not client:
            self.logger.warning("‚ö†Ô∏è No Airweave client available for collection cleanup")
            return

        cleanup_stats = {"collections_deleted": 0, "errors": 0}

        try:
            # Find all test collections
            test_collections = await self._find_test_collections(client)

            if test_collections:
                self.logger.info(f"üîç Found {len(test_collections)} test collections to clean up")

                for collection in test_collections:
                    try:
                        client.collections.delete(collection["readable_id"])
                        cleanup_stats["collections_deleted"] += 1
                        self.logger.info(
                            f"‚úÖ Deleted collection: {collection['name']} ({collection['readable_id']})"
                        )
                    except Exception as e:
                        cleanup_stats["errors"] += 1
                        self.logger.warning(
                            f"‚ö†Ô∏è Failed to delete collection {collection['readable_id']}: {e}"
                        )

            # Log cleanup summary
            self.logger.info(
                f"üßπ Collection cleanup completed: {cleanup_stats['collections_deleted']} collections deleted, "
                f"{cleanup_stats['errors']} errors"
            )

        except Exception as e:
            self.logger.error(f"‚ùå Error during collection cleanup: {e}")
            # Don't re-raise - cleanup should be best-effort

    def _get_airweave_client(self) -> Any:
        return getattr(self.config, "_airweave_client", None)

    async def _find_test_collections(self, client: Any) -> List[Dict[str, Any]]:
        """Find all test collections that should be cleaned up."""
        test_collections = []

        try:
            # Get all collections
            collections = client.collections.list()

            # Convert to list if it's a generator or iterator
            if hasattr(collections, "__iter__") and not isinstance(collections, list):
                collections = list(collections)

            for collection in collections:
                # Convert to dict if it's a Pydantic model
                if hasattr(collection, "model_dump"):
                    collection_data = collection.model_dump()
                elif hasattr(collection, "dict"):
                    collection_data = collection.dict()
                else:
                    collection_data = (
                        dict(collection) if hasattr(collection, "__dict__") else collection
                    )

                name = collection_data.get("name", "")
                readable_id = collection_data.get("readable_id", "")

                # Check if this looks like a test collection
                is_test_collection = (
                    name.lower().startswith("monke-")
                    or "test" in name.lower()
                    and ("collection" in name.lower() or "monke" in name.lower())
                    or readable_id.startswith("monke-")
                )

                if is_test_collection:
                    test_collections.append(collection_data)

        except Exception as e:
            self.logger.error(f"‚ùå Error finding test collections: {e}")

        return test_collections


class TestStepFactory:
    """Factory for creating test steps."""

    _steps = {
        "cleanup": CleanupStep,
        "collection_cleanup": CollectionCleanupStep,
        "create": CreateStep,
        "sync": SyncStep,
        "verify": VerifyStep,
        "update": UpdateStep,
        "partial_delete": PartialDeleteStep,
        "verify_partial_deletion": VerifyPartialDeletionStep,
        "verify_remaining_entities": VerifyRemainingEntitiesStep,
        "complete_delete": CompleteDeleteStep,
        "verify_complete_deletion": VerifyCompleteDeletionStep,
    }

    def create_step(self, step_name: str, config: TestConfig) -> TestStep:
        if step_name not in self._steps:
            raise ValueError(f"Unknown test step: {step_name}")
        return self._steps[step_name](config)
